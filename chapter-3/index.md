# Recurrent Neural Networks

- intro
  - have memory, can use past inputs to generate current outputs
  - temporal dependencies : involve current and past inputs
- history
  - temporal data in world is abundant in video, speech, text, and feedforward was bad at capturing temporal relationships
  - time delay NN : tried to capture temporal info, but was limited to set time window size
  - simple RNN (elman network) : suffered from vanishing gradient problem
  - vanishing gradient problem : cannot capture information 8-10 steps backward, due to backpropagation and the fact that the gradient is multiplied by derivatives, if derivatives are <1, a convergent geometric series is created and the gradient diminishes rapidly
  - LSTM : address vanishing gradient problem
- applications
  - speech recognition
  - time series prediction (traffic = waze, movie selection = netflix, stock market conditions = hedge funds)
  - NLP (translation)
  - gesture recognition (recognizing human gesture from video)
- differences (RNN and normal NN)
  - train with sequences : previous input matters
  - memory elements : output of hidden layer that serve as input to network at next train step
  - s (not h) : for hidden layers to represent states of memory
- representing RNNs
  - folded : compact representation where the memory is shown as a state that simply loops back
  - unfolded : less compact representation where each output is shown connect to hidden nodes, and hidden nodes connect to corresponding inputs and previous hidden nodes (showing time at $$t-1,t,t+1,\ldots$$)
  - Diagram: ![ml-notes_22](/images/ml-notes_22.png)
  - $$\bar{x}_{t}$$ : inputs at time $$t$$
  - $$\bar{y}_{t}$$ : outputs at time $$t$$
  - $$\bar{s}_{t}$$ : state (memory) at time $$t$$
  - $$W_{x}$$ : weights of input to state
  - $$W_{s}$$ : weights of previous state to current state
  - $$W_{y}$$ : weights of state to output respectively
  - $$\phi$$ : arbitrary activation function
  - $$\bar{s}_{t}=\phi(\bar{x}_{t}\cdot W_{x} + \bar{s}_{t-1}\cdot W_{s})$$
  - $$\bar{y}_{t}=\bar{s}_{t}\cdot W_{y}$$
- RNN variations
  - CNN/RNN : first few layers conv, flatten, then LSTM recurrent layers, and you can have convolutional neural networks that have memory
  - RNN with multiple inputs or outputs, RNN connected to another RNN
- misc
